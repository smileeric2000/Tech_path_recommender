#!/usr/bin/env python
# coding: utf-8

# # Personalized learning path 
# 
# _This model/app allows Users to uploads their resume or selects current skills ‚Üí app recommends personalized learning paths_

# **Tools**
# 
# üß† NLP (for resume/skills parsing)
# 
# üéØ Similarity matching (cosine similarity or embedding-based)
# 
# üìä Streamlit (frontend)
# 
# üìö Predefined learning paths dataset (you'll create this)
# 
# üîç Optional: LLM or Flan-T5 for suggesting learning goals
learning_recommender/
‚îÇ
‚îú‚îÄ‚îÄ app.py                   # Main Streamlit app
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ learning_paths.json  # Skills mapped to paths
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ parser.py            # Resume parser
‚îÇ   ‚îî‚îÄ‚îÄ recommender.py       # Recommender logic
‚îú‚îÄ‚îÄ sample_resume.txt
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md

# #### Define Learning Paths

# In[1]:


import json
text = {
  "python": {
    "resources": [
      {
        "title": "Complete Python for Beginners",
        "platform": "Coursera",
        "time_estimate": "20 hours",
        "link": "https://www.coursera.org/learn/python"
      },
      {
        "title": "Automate the Boring Stuff with Python",
        "platform": "Book",
        "time_estimate": "15 hours",
        "link": "https://automatetheboringstuff.com/"
      }
    ],
    "prerequisites": []
  },
  "machine learning": {
    "resources": [
      {
        "title": "Machine Learning by Andrew Ng",
        "platform": "Coursera",
        "time_estimate": "55 hours",
        "link": "https://www.coursera.org/learn/machine-learning"
      },
      {
        "title": "Hands-On Machine Learning",
        "platform": "Book",
        "time_estimate": "40 hours",
        "link": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/"
      }
    ],
    "prerequisites": ["python", "data science"]
  }
}


# Save to JSON
with open("learning_paths.json", "w") as f:
    json.dump(text, f, indent=2)


# #### Step 2 Resume or Skill Input
# 
# * Define Resume upload function (Allow Resume to be of types-> (.txt or .pdf)
# 
# * Manual Skill Selection (multi-select box)

# In[2]:


#Import necessary libraries and dependencies
import streamlit as st
from utils.parser import extract_skills
from utils.recommender import recommend_paths


# NOTE :
# 
#     utils ‚Üí package (folder with __init__.py)    
#     parser.py ‚Üí module (a single Python file inside utils)
#     extract_skills ‚Üí function (defined inside the parser.py module)
# 
# The "extract_skills" ‚Äî in utils/parser.py
# Parses a given text (e.g., user input) and extracts known skills mentioned in it.
# How it works:
# 
#     Checks the input text against a predefined list of skills and returns all the skills found.
#     
# The "recommend_paths" ‚Äî in recommender.py
# Takes a list of skills the user already knows and recommends personalized learning paths and resources.
# How it works:
# 
#     For each known skill, it provides relevant learning resources.
#     Identifies ‚Äúnext skills‚Äù the user should learn based on prerequisite relationships and recommends resources for those too.
# 
# Output:
# A dictionary mapping each skill (both current and recommended next skills) to a list of learning resources (e.g., courses, books).
# 
# 

# In[9]:


st.title("üìò Personalized Learning Path Recommender")

skills = []  #Initialize skills <<empty list of skills>>

option = st.radio("Choose Input Type", ["Upload Resume", "Select Skills"])

if option == "Upload Resume":
    uploaded_file = st.file_uploader("Upload your resume", type=["pdf", "txt"])
    if uploaded_file:
        skills = extract_skills(uploaded_file)
        st.write("**Extracted Skills:**", skills)

elif option == "Select Skills":
    all_skills = ["Python", "SQL", "Pandas", "Scikit-learn", "Spark", "Airflow", "Numpy", "Transformers"]
    skills = st.multiselect("Select your current skills", all_skills)

if skills:
    recommendations = recommend_paths(skills)
    st.write("### üéØ Recommended Paths:")
    for rec in recommendations:
        st.markdown(f"- **{rec}**")


# #### Step 3 Extract Skills (NLP Resume Parser)

# #### Step 4 Recommend Based on Skill Overlap

# In[10]:


import json

def recommend_paths(user_skills):
    with open("data/learning_paths.json", "r") as f:
        paths = json.load(f)

    scores = {}
    for path, details in paths.items():
        skill_list = details.get("prerequisites", [])
        overlap = set(user_skills).intersection(set(skill_list))
        scores[path] = len(overlap) / len(skill_list) if skill_list else 0

    return sorted(scores, key=scores.get, reverse=True)


# #### Step 6: (Optional) Suggest Next Skills to Learn
# 
# Extend recommend_paths() to also return:
#     
#     Unlearned skills per path
#     Time estimates
#     Resources (links from Medium, YouTube, etc.)
# 

# #### Step 7 LLM Integration ( Skill-to-Course Match Using APIs)
# 
#     Using HuggingFace‚Äôs OpenAI GPT API.
# 
#     Fetch real-time course data from platforms like Coursera, EdX, Udemy.
# 
# 
#     
#     Input: user skills + recommended learning path.
#     
#     Output: list of course names + short description.

# ##### Coursera API fetch function

# In[11]:


import requests

def fetch_coursera_courses(query, limit=5):
    """
    Fetch top courses from Coursera matching the query.
    """
    url = "https://api.coursera.org/api/courses.v1"
    params = {
        "q": "search",
        "query": query,
        "limit": limit,
        "fields": "name,description,slug"
    }
    response = requests.get(url, params=params)
    data = response.json()

    courses = []
    for course in data.get("elements", []):
        name = course.get("name")
        slug = course.get("slug")
        url = f"https://www.coursera.org/learn/{slug}" if slug else ""
        courses.append({"name": name, "url": url})
    return courses


# ##### OpenAI GPT API Integration

# In[12]:


# #set OpenAI API key environment variables 

# import openai
# import os

# openai.api_key = os.getenv("OPENAI_API_KEY")  # Or set directly here

# def generate_course_recommendations(user_skills, learning_path, experience_level):
#     prompt = f"""
#     You are a helpful learning advisor.

#     User has skills: {', '.join(user_skills)}.
#     They want to learn: {learning_path}.
#     Their experience level is: {experience_level}.

#     Suggest 3 online courses (with titles) suitable for this user on platforms like Coursera, EdX, or Udemy.
#     Provide short descriptions for each course.
#     """

#     response = openai.ChatCompletion.create(
#         model="gpt-4o-mini",
#         messages=[{"role": "user", "content": prompt}],
#         temperature=0.7,
#         max_tokens=300,
#     )

#     return response.choices[0].message.content.strip()


# ##### PDF Export with fpdf

# In[13]:


from fpdf import FPDF

def create_pdf_report(user_skills, experience_level, recommended_paths, course_recommendations):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", "B", 16)
    pdf.cell(0, 10, "Personalized Learning Path Report", ln=True)
    pdf.set_font("Arial", "", 12)
    pdf.cell(0, 10, f"Skills: {', '.join(user_skills)}", ln=True)
    pdf.cell(0, 10, f"Experience Level: {experience_level}", ln=True)
    pdf.ln(10)

    pdf.set_font("Arial", "B", 14)
    pdf.cell(0, 10, "Recommended Learning Paths:", ln=True)
    pdf.set_font("Arial", "", 12)
    for path in recommended_paths:
        pdf.cell(0, 10, f"- {path}", ln=True)

    pdf.ln(10)
    pdf.set_font("Arial", "B", 14)
    pdf.cell(0, 10, "Course Recommendations:", ln=True)
    pdf.set_font("Arial", "", 12)
    for line in course_recommendations.split("\n"):
        pdf.cell(0, 10, line, ln=True)

    filename = "learning_path_report.pdf"
    pdf.output(filename)
    return filename


# ##  Streamlit Full App Example (app.py)

# In[14]:


from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
from pdf_generator import create_pdf_report, fetch_coursera_courses, generate_course_recommendations

# Load GPT-2 model and tokenizer
@st.cache_resource
def load_gpt2_model():
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    model.eval()
    return tokenizer, model

tokenizer, model = load_gpt2_model()

@st.cache_data
def load_learning_paths():
    with open("data/learning_paths.json", "r") as f:
        return json.load(f)

def extract_skills_from_text(text, keywords):
    return [kw for kw in keywords if re.search(rf"\b{kw}\b", text, re.IGNORECASE)]

def gpt2_generate(prompt, max_length=100):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_length=input_ids.shape[1] + max_length,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.7
        )
    generated = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated[len(prompt):].strip().split("User:")[0].strip()

def main():
    st.title("üìò Personalized Learning Path Recommender (Offline GPT-2 Version)")

    learning_paths = load_learning_paths()
    all_skills = sorted({skill for skills in learning_paths.values() for skill in skills})

    input_type = st.radio("Choose input type", ["Upload Resume", "Select Skills"])
    experience_level = st.selectbox("Select your experience level", ["Beginner", "Intermediate", "Advanced"])

    user_skills = []

    if input_type == "Upload Resume":
        uploaded_file = st.file_uploader("Upload your resume (.txt only)", type=["txt"])
        if uploaded_file:
            text = uploaded_file.read().decode("utf-8")
            user_skills = extract_skills_from_text(text, all_skills)
            st.write("### Extracted Skills:")
            st.write(user_skills if user_skills else "No known skills detected. Try manual selection.")

    elif input_type == "Select Skills":
        user_skills = st.multiselect("Select your current skills", all_skills)

    if user_skills:
        scores = {}
        for path, skills in learning_paths.items():
            overlap = set(user_skills).intersection(skills)
            scores[path] = len(overlap) / len(skills)
        recommended_paths = sorted(scores, key=scores.get, reverse=True)

        st.write("### üéØ Recommended Learning Paths:")
        for path in recommended_paths:
            st.markdown(f"**{path}** ‚Äî Skills: {', '.join(learning_paths[path])}")

        if st.button("Get Course Recommendations"):
            with st.spinner("Generating personalized suggestions..."):
                prompt = (
                    f"Suggest personalized courses for someone with the following skills: {', '.join(user_skills)}. "
                    f"The recommended learning path is: {recommended_paths[0]}. "
                    f"Their experience level is {experience_level}.\nCourses:"
                )
                course_recs = gpt2_generate(prompt)
                st.markdown("### üìö Course Recommendations:")
                st.write(course_recs)

                courses = fetch_coursera_courses(recommended_paths[0])
                st.markdown("### üîé Real Coursera Courses:")
                for course in courses:
                    st.markdown(f"- [{course['name']}]({course['url']})")

                if st.button("Download PDF Report"):
                    filename = create_pdf_report(user_skills, experience_level, recommended_paths, course_recs)
                    with open(filename, "rb") as f:
                        st.download_button(label="Download PDF", data=f, file_name=filename, mime="application/pdf")

if __name__ == "__main__":
    main()


# In[ ]:




